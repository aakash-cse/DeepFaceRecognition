{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\";>Nuclei Technologies</h1>\n",
    "<h2 style=\"text-align:right\";>Done by B.Aakash<h2>\n",
    "\n",
    "<h3>Problem Statement</h3>\n",
    "\n",
    "<p>We want you to implement a face recognition system, don't worry we have a tutorial to guide you through it as well. Follow this video and try to implement a Face Recognition system using the pretrained model, you can use your own sample images from the internet to try it.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.camera-module.com/upfile/2019/06/20190616212440_891.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for that we need to create a Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Person's Name ....Aakash_validation\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "name = input(\"Enter the Person's Name ....\")\n",
    "\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Cropping the images    \n",
    "    for (x,y,w,h) in faces:\n",
    "        x=x-10\n",
    "        y=y-10\n",
    "        cropped_face = img[y:y+h+50, x:x+w+50]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0 # for convenience for us to save the images so that it doesn't save the same file\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (400, 400))\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './Dataset/' + name +\"-\"+ str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        #print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since I trained the model in kaggle for faster computation i have uploaded the dataset in kaggle and computed as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle link here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets us predict the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_image(filename):\n",
    "    img = load_img(filename, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    img = img.astype('float32')\n",
    "    return img\n",
    "\n",
    "def run_example(path):\n",
    "    img = load_image(path)\n",
    "    # load model\n",
    "    model = load_model('trained-model.h5')\n",
    "    # predict the class\n",
    "    result = model.predict(img)\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# entry point, run the example\n",
    "run_example('Dataset/Aakash_validation-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000000e+00 4.5556967e-26 2.7625276e-33]\n"
     ]
    }
   ],
   "source": [
    "run_example(\"Dataset/Test/Raghava/Raghava_test-1.jpg\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
